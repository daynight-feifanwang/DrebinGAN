import os
import time
import numpy as np
import torch
import logging
from torch.utils.data import Dataset, DataLoader
import torch.autograd as autograd
import torch.optim as optim
from torch.autograd import Variable
import utils
from visdom import Visdom
from model import Generator, Discriminator

logging.basicConfig(level=logging.INFO, format="'%(asctime)s - %(name)s: %(levelname)s: %(message)s'")
logger = logging.getLogger("DrebinGAN.STDOUT")
logger.setLevel("INFO")

torch.manual_seed(10)
use_cuda = torch.cuda.is_available()

one = torch.tensor(1, dtype=torch.float)
minus_one = one * -1

one = one.cuda() if use_cuda else one
minus_one = minus_one.cuda() if use_cuda else minus_one

# visdom for plotting
vis = Visdom()
win_g, win_d, win_w = None, None, None
assert vis.check_connection()

class DataProcessor():
    def __init__(self, load_path='./middle_data'):
        self.load_path = load_path
        self.features = None
        self.feature_dict = dict()

        self.dataset = None
        self.loader = None
        self.iterator = None

    def load_feature(self):
        self.features = utils.import_from_pkl(self.load_path, 'benign_feature_list.data')

        for i, feature in enumerate(self.features):
            self.feature_dict[feature] = i

        return len(self.features)

    def load_data(self, load_path, file_name, batch_size):
        load_abs_path = utils.get_absolute_path(load_path)
        sample_path = os.path.join(load_abs_path, file_name)

        self.dataset = DrebinDataset(sample_path)
        self.loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)

    def next(self):
        try:
            samples = next(self.iterator)
        except (StopIteration, TypeError):
            self.iter_mal = iter(self.iterator)
            samples = next(self.iterator)
        return samples


class DrebinDataset(Dataset):
    def __init__(self, load_path):
        self.sample_files = np.array([x.path for x in os.scandir(load_path) if x.name.endswith('.feature')])

    def __getitem__(self, index):
        sample = utils.import_from_pkl(self.sample_files[index])
        return torch.BoolTensor(sample)

    def __len__(self):
        return len(self.sample_files)

class WGAN_GP(object):
    def __init__(self, load_path='./middle_data', save_path ='./final_data', model_path='./save_model'):
        self.load_path = load_path
        self.save_path = save_path
        self.model_path = model_path

        # hyper-parameters
        self.model_name = 'DrebinGAN'
        self.batch_size = 1000
        self.dim = 64 # need to adjust
        self.n_dim = 100
        self.n_critic = 5
        self.max_epoch = 10000
        self.lambda_ = 10
        self.lrG = 0.0001
        self.lrD = 0.0001
        self.beta_1 = 0.5
        self.beta_2 = 0.999

        # data loader
        self.DP = DataProcessor(load_path)
        self.feature_size = self.DP.load_feature()
        logger.info("Feature size: {}".format(self.feature_size))

        # gen and dis module
        self.G = Generator(self.dim, self.n_dim, self.feature_size, 2)
        self.D = Discriminator(self.dim, self.feature_size, 2)
        self.G_optim = optim.Adam(self.G.parameters(), lr=self.lrG, betas=(self.beta_1, self.beta_2))
        self.D_optim = optim.Adam(self.D.parameters(), lr=self.lrD, betas=(self.beta_1, self.beta_2))

        if use_cuda:
            self.G.cuda()
            self.D.cuda()

    def train(self):
        logger.info("Loading data...")
        self.DP.load_data(self.load_path, 'good_sample', self.batch_size)

        logger.info("Starting training GAN...")
        self.G.train()
        self.D.train()
        start_time = time.time()
        for epoch in range(self.max_epoch):
            epoch_start_time = time.time()

            # Update Discriminator
            for parameter in self.D.parameters(): # reset requires_grad every time
                parameter.requires_grad = True    # as they are blocked during Generator updating.
            for i_critic in range(self.n_critic):
                self.D.zero_grad()

                # get good samples
                # TODO 可能需要成 one-hot ？？？
                real_data = self.DP.next() # real_data = {ndarray: (batch_size, feat_size)}
                real_data = real_data.cuda() if use_cuda else real_data
                real_data_v = Variable(real_data)

                # train discriminator with real data
                D_real = self.D(real_data_v)
                D_real = D_real.mean()
                D_real.backward(minus_one)

                # train discriminator with fake data generated by generator
                noise = torch.randn(self.batch_size, self.n_dim)
                noise = noise.cuda() if use_cuda else noise
                noise = Variable(noise, volatile=True)

                G_fake = self.G(noise)
                G_fake = Variable(G_fake.data)

                D_fake = self.D(G_fake)
                D_fake = D_fake.mean()
                D_fake.backward(one)

                # update with grdient penalty
                gp = self.compute_gradient_penalty(D_real, D_fake)
                gp.backward()


                D_loss = D_fake - D_real + gp
                D_wasserstein = D_real - D_fake
                self.D_optim.step()

            # Update Generator
            for parameter in self.D.parameters():
                parameter.requires_grad = False
            self.G.zero_grad()

            noise = torch.randn(self.batch_size, self.n_dim)
            noise = noise.cuda() if use_cuda else noise
            noise = Variable(noise)

            G_fake = self.G(noise)
            D_fake = self.D(G_fake)
            D_fake = D_fake.mean()
            D_fake.backward(minus_one)

            G_loss = -D_fake
            self.G_optim.step()

            # print log
            # Logger.info("{} - G_loss: {}\tD_loss: {}".format(epoch, G_epoch_loss, D_epoch_loss / self.n_critic))
            print('[{}] - D_loss: {}, G_loss: {} ... epoch_time: {}'.format(epoch, D_loss.data[0],
                                                                            G_loss.data[0],
                                                                            D_wasserstein.data[0],
                                                                            utils.consume_time(
                                                                                time.time() - epoch_start_time)
                                                                            ))
            # plot log
            win_d = utils.plot('Discriminator Loss', vis, x=epoch, y=D_loss.data[0], win=win_d)
            win_g = utils.plot('Generator Loss', vis, x=epoch, y=G_loss.data[0], win=win_g)
            win_w = utils.plot('Wasserstein Distance', vis, x=epoch, y=D_wasserstein.data[0], win=win_w)

            # save model and generate samples every 100 epochs
            if epoch % 100 == 99:
                # generate samples
                samples = self.generate(saving=True)
                # self.evaluate(samples)
            if epoch % 100 == 99:
                # save model
                print('[{}] - Saving model'.format(epoch))
                self.save(epoch)

    def save(self, epoch):
        if not os.path.exists(self.model_path):
            os.makedirs(self.model_path)

        torch.save(self.G.state_dict(), os.path.join(self.model_path, self.model_name + '_G_{:d}.pkl'.format(epoch)))
        torch.save(self.D.state_dict(), os.path.join(self.model_path, self.model_name + '_D_{:d}.pkl'.format(epoch)))

    def load(self, G_file_name, D_file_name):
        self.G.load_state_dict(torch.load(os.path.join(self.model_path, G_file_name)))
        self.D.load_state_dict(torch.load(os.path.join(self.model_path, D_file_name)))

    def compute_gradient_penalty(self, D_real, D_fake):
        alpha = torch.rand(self.batch_size, 1, 1).expand(D_real.size())
        alpha = alpha.cuda() if use_cuda else alpha

        interpolates = alpha * D_real + (1 - alpha) * D_fake
        interpolates = interpolates.cuda() if use_cuda else alpha
        interpolates = autograd.Variable(interpolates, requires_grad=True)

        D_inter = self.D(interpolates)

        ones = torch.ones(D_inter.size()).cuda() if use_cuda else torch.ones(D_inter.size())

        gradients = autograd.grad(outputs=D_inter,
                                  inputs=interpolates,
                                  grad_outputs=ones,
                                  create_graph=True,
                                  retain_graph=True,
                                  only_inputs=True)[0]
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.lambda_
        return gradient_penalty

    def generate(self, num=1, saving=False, save_path=None):
        noise = torch.randn(num, self.n_dim)
        noise = noise.cuda() if use_cuda else noise
        noise = Variable(noise, volatile=True)

        samples = self.G(noise)
        samples = samples.view(-1, self.feature_size, 2)
        _, samples = torch.max(samples, 2)
        samples = samples.cpu().data

        if saving and save_path:
            utils.export_to_pkl(save_path, content=samples)
        return samples

    def evaluate(self, samples):
        pass


if __name__ == '__main__':
    gan = WGAN_GP()
    gan.train()
